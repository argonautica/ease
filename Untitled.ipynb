{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "073e011a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def convert_unix_timestamps(df):\n",
    "    # Convert the timestamp column to datetime\n",
    "    df['TIMESTAMP'] = pd.to_datetime(df['TIMESTAMP'], unit='s')\n",
    "\n",
    "    # Extract day of the year\n",
    "    df['Day_of_Year'] = df['TIMESTAMP'].dt.dayofyear\n",
    "\n",
    "    # Extract day of the week (Monday is 0 and Sunday is 6)\n",
    "    df['Day_of_Week'] = df['TIMESTAMP'].dt.dayofweek\n",
    "\n",
    "    # Extract hour of the day\n",
    "    df['Hour_of_Day'] = df['TIMESTAMP'].dt.hour\n",
    "\n",
    "    # Extract minute of the hour\n",
    "    df['Minute_of_Hour'] = df['TIMESTAMP'].dt.minute\n",
    "\n",
    "    # Calculate the week of the year\n",
    "    df['Week_of_Year'] = df['TIMESTAMP'].dt.isocalendar().week\n",
    "\n",
    "    # Calculate the 15-minute segment of the day\n",
    "    df['Segment_of_Day'] = (df['TIMESTAMP'].dt.minute // 15) + 1\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41e3212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CALL_TYPE' 'ORIGIN_CALL' 'ORIGIN_STAND' 'TAXI_ID' 'TIMESTAMP' 'DAY_TYPE'\n",
      " 'Day_of_Year' 'Day_of_Week' 'Hour_of_Day' 'Minute_of_Hour' 'Week_of_Year'\n",
      " 'Segment_of_Day']\n",
      "['CALL_TYPE' 'ORIGIN_CALL' 'ORIGIN_STAND' 'TAXI_ID' 'TIMESTAMP' 'DAY_TYPE'\n",
      " 'Day_of_Year' 'Day_of_Week' 'Hour_of_Day' 'Minute_of_Hour' 'Week_of_Year'\n",
      " 'Segment_of_Day']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv(\"train_real.csv\")\n",
    "test = pd.read_csv(\"test_public.csv\")\n",
    "\n",
    "\n",
    "train = convert_unix_timestamps(train)\n",
    "test = convert_unix_timestamps(test)\n",
    "\n",
    "\n",
    "train = train.drop([\"TRIP_ID\", \"LAT\", \"LONG\", \"MISSING_DATA\"], axis=1)\n",
    "y = train.pop(\"TIME\")\n",
    "\n",
    "test = test.drop([\"MISSING_DATA\"], axis=1)\n",
    "testids = test.pop(\"TRIP_ID\")\n",
    "print(train.columns.values)\n",
    "print(test.columns.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c308b723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique TAXI_ID values in train not in test: 204\n",
      "Number of unique ORIGIN_CALL values in train not in test: 57065\n",
      "Number of unique ORIGIN_STAND values in train not in test: 25\n",
      "Number of unique TAXI_ID values in test not in train: 0\n",
      "Number of unique ORIGIN_CALL values in test not in train: 20\n",
      "Number of unique ORIGIN_STAND values in test not in train: 1\n"
     ]
    }
   ],
   "source": [
    "# Find unique values in train and test for each column\n",
    "train_taxi_id_unique = set(train['TAXI_ID'].unique())\n",
    "test_taxi_id_unique = set(test['TAXI_ID'].unique())\n",
    "\n",
    "train_origin_call_unique = set(train['ORIGIN_CALL'].unique())\n",
    "test_origin_call_unique = set(test['ORIGIN_CALL'].unique())\n",
    "\n",
    "train_origin_stand_unique = set(train['ORIGIN_STAND'].unique())\n",
    "test_origin_stand_unique = set(test['ORIGIN_STAND'].unique())\n",
    "\n",
    "# Calculate the number of values in train that are not in test\n",
    "taxi_id_not_in_test = len(train_taxi_id_unique - test_taxi_id_unique)\n",
    "origin_call_not_in_test = len(train_origin_call_unique - test_origin_call_unique)\n",
    "origin_stand_not_in_test = len(train_origin_stand_unique - test_origin_stand_unique)\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of unique TAXI_ID values in train not in test:\", taxi_id_not_in_test)\n",
    "print(\"Number of unique ORIGIN_CALL values in train not in test:\", origin_call_not_in_test)\n",
    "print(\"Number of unique ORIGIN_STAND values in train not in test:\", origin_stand_not_in_test)\n",
    "\n",
    "\n",
    "# Find unique values in train and test for each column\n",
    "train_taxi_id_unique = set(train['TAXI_ID'].unique())\n",
    "test_taxi_id_unique = set(test['TAXI_ID'].unique())\n",
    "\n",
    "train_origin_call_unique = set(train['ORIGIN_CALL'].unique())\n",
    "test_origin_call_unique = set(test['ORIGIN_CALL'].unique())\n",
    "\n",
    "train_origin_stand_unique = set(train['ORIGIN_STAND'].unique())\n",
    "test_origin_stand_unique = set(test['ORIGIN_STAND'].unique())\n",
    "\n",
    "# Calculate the number of values in test that are not in train\n",
    "taxi_id_not_in_train = len(test_taxi_id_unique - train_taxi_id_unique)\n",
    "origin_call_not_in_train = len(test_origin_call_unique - train_origin_call_unique)\n",
    "origin_stand_not_in_train = len(test_origin_stand_unique - train_origin_stand_unique)\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of unique TAXI_ID values in test not in train:\", taxi_id_not_in_train)\n",
    "print(\"Number of unique ORIGIN_CALL values in test not in train:\", origin_call_not_in_train)\n",
    "print(\"Number of unique ORIGIN_STAND values in test not in train:\", origin_stand_not_in_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e5ec5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set NaN values to zero in train DataFrame\n",
    "train.fillna(0, inplace=True)\n",
    "\n",
    "# Set NaN values to zero in test DataFrame\n",
    "test.fillna(0, inplace=True)\n",
    "\n",
    "# Combine train and test DataFrames\n",
    "combined = pd.concat([train, test], axis=0)\n",
    "\n",
    "# Encode 'CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', and 'TAXI_ID' consistently\n",
    "for column in ['CALL_TYPE', 'ORIGIN_CALL', 'ORIGIN_STAND', 'TAXI_ID']:\n",
    "    combined[column] = pd.Categorical(combined[column], categories=combined[column].unique(), ordered=False)\n",
    "    combined[column] = combined[column].cat.codes\n",
    "\n",
    "# Split the combined DataFrame back into train and test DataFrames\n",
    "train = combined[:len(train)]\n",
    "test = combined[len(train):]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f082069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'TIMESTAMP' column from train DataFrame\n",
    "train = train.drop(['TIMESTAMP', 'DAY_TYPE'], axis=1)\n",
    "\n",
    "# Drop 'TIMESTAMP' column from test DataFrame\n",
    "test = test.drop(['TIMESTAMP','DAY_TYPE'], axis=1)\n",
    "\n",
    "\n",
    "# y = y.multiply(15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a467500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1710670, 10)\n",
      "['CALL_TYPE' 'ORIGIN_CALL' 'ORIGIN_STAND' 'TAXI_ID' 'Day_of_Year'\n",
      " 'Day_of_Week' 'Hour_of_Day' 'Minute_of_Hour' 'Week_of_Year'\n",
      " 'Segment_of_Day']\n",
      "['CALL_TYPE' 'ORIGIN_CALL' 'ORIGIN_STAND' 'TAXI_ID' 'Day_of_Year'\n",
      " 'Day_of_Week' 'Hour_of_Day' 'Minute_of_Hour' 'Week_of_Year'\n",
      " 'Segment_of_Day']\n",
      "(1704769, 10)\n",
      "(1704769,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(train.shape)\n",
    "\n",
    "# train = train.drop('DAY_TYPE', axis=1)\n",
    "\n",
    "# test = test.drop('DAY_TYPE', axis=1)\n",
    "print(train.columns.values)\n",
    "print(test.columns.values)\n",
    "\n",
    "\n",
    "dropped_indices = y[y <= 0].index\n",
    "y = y.drop(dropped_indices)\n",
    "\n",
    "# Drop the corresponding values in train\n",
    "train = train.drop(dropped_indices)\n",
    "\n",
    "print(train.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "839b5748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import ast\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e5df272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND  TAXI_ID  Day_of_Year  Day_of_Week  \\\n",
      "0          0            0             0        0          182            0   \n",
      "1          1            0             1        1          182            0   \n",
      "2          0            0             0        2          182            0   \n",
      "3          0            0             0        3          182            0   \n",
      "4          0            0             0        4          182            0   \n",
      "\n",
      "   Hour_of_Day  Minute_of_Hour  Week_of_Year  Segment_of_Day  \n",
      "0            0               0            27               1  \n",
      "1            0               8            27               1  \n",
      "2            0               2            27               1  \n",
      "3            0               0            27               1  \n",
      "4            0               4            27               1  \n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "print(train.head())\n",
    "\n",
    "\n",
    "# y = y.multiply(15)\n",
    "\n",
    "# print(type(train))\n",
    "# print(type(y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b6df836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Drop 'ORIGIN_CALL' and 'ORIGIN_STAND' columns from train dataframe\n",
    "train = train.drop(['ORIGIN_CALL', 'ORIGIN_STAND'], axis=1)\n",
    "\n",
    "# Drop 'ORIGIN_CALL' and 'ORIGIN_STAND' columns from test dataframe\n",
    "test = test.drop(['ORIGIN_CALL', 'ORIGIN_STAND'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9afe44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1687721, 10]) torch.Size([17048, 10])\n",
      "Epoch [10/4000], Loss: 52.8444\n",
      "RMSE: 76.9470\n",
      "Epoch [20/4000], Loss: 52.3222\n",
      "RMSE: 50.1566\n",
      "Epoch [30/4000], Loss: 48.1264\n",
      "RMSE: 50.0640\n",
      "Epoch [40/4000], Loss: 47.8892\n",
      "RMSE: 48.4234\n",
      "Epoch [50/4000], Loss: 47.7666\n",
      "RMSE: 48.1646\n",
      "Epoch [60/4000], Loss: 47.2089\n",
      "RMSE: 48.0309\n",
      "Epoch [70/4000], Loss: 47.0041\n",
      "RMSE: 47.8519\n",
      "Epoch [80/4000], Loss: 46.8401\n",
      "RMSE: 47.6552\n",
      "Epoch [90/4000], Loss: 46.7482\n",
      "RMSE: 47.6162\n",
      "Epoch [100/4000], Loss: 46.6825\n",
      "RMSE: 47.4656\n",
      "Epoch [110/4000], Loss: 46.5948\n",
      "RMSE: 47.3887\n",
      "Epoch [120/4000], Loss: 46.5151\n",
      "RMSE: 47.3139\n",
      "Epoch [130/4000], Loss: 46.4580\n",
      "RMSE: 47.2521\n",
      "Epoch [140/4000], Loss: 46.5050\n",
      "RMSE: 47.4048\n",
      "Epoch [150/4000], Loss: 46.3775\n",
      "RMSE: 47.2718\n",
      "Epoch [160/4000], Loss: 46.3792\n",
      "RMSE: 47.1462\n",
      "Epoch [170/4000], Loss: 46.2962\n",
      "RMSE: 47.0936\n",
      "Epoch [180/4000], Loss: 46.2402\n",
      "RMSE: 47.0325\n",
      "Epoch [190/4000], Loss: 46.1808\n",
      "RMSE: 47.0010\n",
      "Epoch [200/4000], Loss: 46.4838\n",
      "RMSE: 47.4358\n",
      "Epoch [210/4000], Loss: 48.9630\n",
      "RMSE: 48.8591\n",
      "Epoch [220/4000], Loss: 46.8605\n",
      "RMSE: 47.2016\n",
      "Epoch [230/4000], Loss: 46.2043\n",
      "RMSE: 47.1356\n",
      "Epoch [240/4000], Loss: 46.1699\n",
      "RMSE: 46.9317\n",
      "Epoch [250/4000], Loss: 46.0966\n",
      "RMSE: 46.8453\n",
      "Epoch [260/4000], Loss: 46.0333\n",
      "RMSE: 46.8011\n",
      "Epoch [270/4000], Loss: 45.9814\n",
      "RMSE: 46.7598\n",
      "Epoch [280/4000], Loss: 45.9411\n",
      "RMSE: 46.7143\n",
      "Epoch [290/4000], Loss: 45.8967\n",
      "RMSE: 46.6751\n",
      "Epoch [300/4000], Loss: 45.8554\n",
      "RMSE: 46.6517\n",
      "Epoch [310/4000], Loss: 45.8334\n",
      "RMSE: 46.6897\n",
      "Epoch [320/4000], Loss: 45.8004\n",
      "RMSE: 46.6538\n",
      "Epoch [330/4000], Loss: 46.1137\n",
      "RMSE: 47.2234\n",
      "Epoch [340/4000], Loss: 46.9099\n",
      "RMSE: 47.3832\n",
      "Epoch [350/4000], Loss: 46.3262\n",
      "RMSE: 47.1094\n",
      "Epoch [360/4000], Loss: 46.0564\n",
      "RMSE: 46.8967\n",
      "Epoch [370/4000], Loss: 45.9114\n",
      "RMSE: 46.6123\n",
      "Epoch [380/4000], Loss: 45.7748\n",
      "RMSE: 46.5253\n",
      "Epoch [390/4000], Loss: 45.7271\n",
      "RMSE: 46.4804\n",
      "Epoch [400/4000], Loss: 45.6933\n",
      "RMSE: 46.4523\n",
      "Epoch [410/4000], Loss: 45.6655\n",
      "RMSE: 46.4295\n",
      "Epoch [420/4000], Loss: 45.6523\n",
      "RMSE: 46.4277\n",
      "Epoch [430/4000], Loss: 45.6932\n",
      "RMSE: 46.4735\n",
      "Epoch [440/4000], Loss: 46.0400\n",
      "RMSE: 47.7411\n",
      "Epoch [450/4000], Loss: 46.0929\n",
      "RMSE: 46.9459\n",
      "Epoch [460/4000], Loss: 45.7971\n",
      "RMSE: 46.6221\n",
      "Epoch [470/4000], Loss: 45.8967\n",
      "RMSE: 46.6677\n",
      "Epoch [480/4000], Loss: 46.0003\n",
      "RMSE: 46.5229\n",
      "Epoch [490/4000], Loss: 46.0150\n",
      "RMSE: 46.6316\n",
      "Epoch [500/4000], Loss: 45.7321\n",
      "RMSE: 46.5615\n",
      "Epoch [510/4000], Loss: 45.6519\n",
      "RMSE: 46.4984\n",
      "Epoch [520/4000], Loss: 45.6387\n",
      "RMSE: 46.4137\n",
      "Epoch [530/4000], Loss: 45.6137\n",
      "RMSE: 46.3997\n",
      "Epoch [540/4000], Loss: 46.6173\n",
      "RMSE: 48.5490\n",
      "Epoch [550/4000], Loss: 45.9506\n",
      "RMSE: 47.1828\n",
      "Epoch [560/4000], Loss: 45.7794\n",
      "RMSE: 46.7647\n",
      "Epoch [570/4000], Loss: 45.7064\n",
      "RMSE: 46.5550\n",
      "Epoch [580/4000], Loss: 45.6900\n",
      "RMSE: 46.4619\n",
      "Epoch [590/4000], Loss: 45.6654\n",
      "RMSE: 46.4390\n",
      "Epoch [600/4000], Loss: 45.6416\n",
      "RMSE: 46.4205\n",
      "Epoch [610/4000], Loss: 45.6293\n",
      "RMSE: 46.4153\n",
      "Epoch [620/4000], Loss: 45.6701\n",
      "RMSE: 46.4619\n",
      "Epoch [630/4000], Loss: 45.6353\n",
      "RMSE: 46.4291\n",
      "Epoch [640/4000], Loss: 45.6013\n",
      "RMSE: 46.3922\n",
      "Epoch [650/4000], Loss: 45.6783\n",
      "RMSE: 46.4235\n",
      "Epoch [660/4000], Loss: 45.6032\n",
      "RMSE: 46.3691\n",
      "Epoch [670/4000], Loss: 45.6324\n",
      "RMSE: 46.3870\n",
      "Epoch [680/4000], Loss: 45.5994\n",
      "RMSE: 46.3603\n",
      "Epoch [690/4000], Loss: 45.5808\n",
      "RMSE: 46.3490\n",
      "Epoch [700/4000], Loss: 45.5800\n",
      "RMSE: 46.3452\n",
      "Epoch [710/4000], Loss: 45.6299\n",
      "RMSE: 46.3835\n",
      "Epoch [720/4000], Loss: 45.6595\n",
      "RMSE: 46.4643\n",
      "Epoch [730/4000], Loss: 45.6485\n",
      "RMSE: 46.3845\n",
      "Epoch [740/4000], Loss: 45.5895\n",
      "RMSE: 46.4640\n",
      "Epoch [750/4000], Loss: 45.6450\n",
      "RMSE: 46.3524\n",
      "Epoch [760/4000], Loss: 46.1170\n",
      "RMSE: 46.4524\n",
      "Epoch [770/4000], Loss: 45.8802\n",
      "RMSE: 46.9173\n",
      "Epoch [780/4000], Loss: 46.2543\n",
      "RMSE: 47.0445\n",
      "Epoch [790/4000], Loss: 45.9445\n",
      "RMSE: 46.6454\n",
      "Epoch [800/4000], Loss: 45.7969\n",
      "RMSE: 46.5432\n",
      "Epoch [810/4000], Loss: 45.6196\n",
      "RMSE: 46.3661\n",
      "Epoch [820/4000], Loss: 45.5330\n",
      "RMSE: 46.2977\n",
      "Epoch [830/4000], Loss: 45.4993\n",
      "RMSE: 46.2880\n",
      "Epoch [840/4000], Loss: 45.4744\n",
      "RMSE: 46.2576\n",
      "Epoch [850/4000], Loss: 45.4697\n",
      "RMSE: 46.2037\n",
      "Epoch [860/4000], Loss: 45.4819\n",
      "RMSE: 46.2153\n",
      "Epoch [870/4000], Loss: 45.4474\n",
      "RMSE: 46.1928\n",
      "Epoch [880/4000], Loss: 45.4083\n",
      "RMSE: 46.2524\n",
      "Epoch [890/4000], Loss: 45.4444\n",
      "RMSE: 46.1500\n",
      "Epoch [900/4000], Loss: 45.5006\n",
      "RMSE: 46.3255\n",
      "Epoch [910/4000], Loss: 45.4228\n",
      "RMSE: 46.1662\n",
      "Epoch [920/4000], Loss: 45.3770\n",
      "RMSE: 46.1728\n",
      "Epoch [930/4000], Loss: 45.3231\n",
      "RMSE: 46.1171\n",
      "Epoch [940/4000], Loss: 45.4934\n",
      "RMSE: 46.1205\n",
      "Epoch [950/4000], Loss: 45.3928\n",
      "RMSE: 46.2066\n",
      "Epoch [960/4000], Loss: 45.4147\n",
      "RMSE: 46.1268\n",
      "Epoch [970/4000], Loss: 45.5315\n",
      "RMSE: 46.3878\n",
      "Epoch [980/4000], Loss: 45.3504\n",
      "RMSE: 46.1527\n",
      "Epoch [990/4000], Loss: 45.6214\n",
      "RMSE: 46.3365\n",
      "Epoch [1000/4000], Loss: 45.3275\n",
      "RMSE: 46.1524\n",
      "Epoch [1010/4000], Loss: 45.2813\n",
      "RMSE: 46.0454\n",
      "Epoch [1020/4000], Loss: 45.2960\n",
      "RMSE: 46.0625\n",
      "Epoch [1030/4000], Loss: 45.7159\n",
      "RMSE: 46.6032\n",
      "Epoch [1040/4000], Loss: 45.8124\n",
      "RMSE: 46.1914\n",
      "Epoch [1050/4000], Loss: 46.2452\n",
      "RMSE: 46.5027\n",
      "Epoch [1060/4000], Loss: 45.5208\n",
      "RMSE: 46.3229\n",
      "Epoch [1070/4000], Loss: 45.3217\n",
      "RMSE: 46.2281\n",
      "Epoch [1080/4000], Loss: 45.3664\n",
      "RMSE: 46.0939\n",
      "Epoch [1090/4000], Loss: 45.2663\n",
      "RMSE: 46.0544\n",
      "Epoch [1100/4000], Loss: 45.2907\n",
      "RMSE: 46.0400\n",
      "Epoch [1110/4000], Loss: 45.2757\n",
      "RMSE: 45.9999\n",
      "Epoch [1120/4000], Loss: 45.2452\n",
      "RMSE: 45.9872\n",
      "Epoch [1130/4000], Loss: 45.2216\n",
      "RMSE: 45.9879\n",
      "Epoch [1140/4000], Loss: 45.2151\n",
      "RMSE: 45.9768\n",
      "Epoch [1150/4000], Loss: 45.2064\n",
      "RMSE: 45.9699\n",
      "Epoch [1160/4000], Loss: 45.1995\n",
      "RMSE: 45.9640\n",
      "Epoch [1170/4000], Loss: 45.2742\n",
      "RMSE: 46.0830\n",
      "Epoch [1180/4000], Loss: 45.2870\n",
      "RMSE: 46.0293\n",
      "Epoch [1190/4000], Loss: 45.2814\n",
      "RMSE: 46.1715\n",
      "Epoch [1200/4000], Loss: 45.3256\n",
      "RMSE: 46.4110\n",
      "Epoch [1210/4000], Loss: 45.3832\n",
      "RMSE: 46.0775\n",
      "Epoch [1220/4000], Loss: 45.2534\n",
      "RMSE: 46.0012\n",
      "Epoch [1230/4000], Loss: 45.4583\n",
      "RMSE: 46.1469\n",
      "Epoch [1240/4000], Loss: 45.2173\n",
      "RMSE: 46.0031\n",
      "Epoch [1250/4000], Loss: 45.1988\n",
      "RMSE: 45.9729\n",
      "Epoch [1260/4000], Loss: 45.1789\n",
      "RMSE: 45.9506\n",
      "Epoch [1270/4000], Loss: 45.1716\n",
      "RMSE: 45.9525\n",
      "Epoch [1280/4000], Loss: 45.2112\n",
      "RMSE: 45.9788\n",
      "Epoch [1290/4000], Loss: 45.1582\n",
      "RMSE: 45.9413\n",
      "Epoch [1300/4000], Loss: 45.3267\n",
      "RMSE: 46.1820\n",
      "Epoch [1310/4000], Loss: 45.2258\n",
      "RMSE: 46.0710\n",
      "Epoch [1320/4000], Loss: 45.2418\n",
      "RMSE: 46.1134\n",
      "Epoch [1330/4000], Loss: 45.2817\n",
      "RMSE: 45.9778\n",
      "Epoch [1340/4000], Loss: 45.1647\n",
      "RMSE: 46.0372\n",
      "Epoch [1350/4000], Loss: 45.3102\n",
      "RMSE: 45.9600\n",
      "Epoch [1360/4000], Loss: 45.1501\n",
      "RMSE: 45.9706\n",
      "Epoch [1370/4000], Loss: 45.1990\n",
      "RMSE: 46.0099\n",
      "Epoch [1380/4000], Loss: 45.1630\n",
      "RMSE: 45.9492\n",
      "Epoch [1390/4000], Loss: 45.1667\n",
      "RMSE: 45.9216\n",
      "Epoch [1400/4000], Loss: 45.1260\n",
      "RMSE: 45.9247\n",
      "Epoch [1410/4000], Loss: 45.3084\n",
      "RMSE: 46.0185\n",
      "Epoch [1420/4000], Loss: 45.1726\n",
      "RMSE: 45.9387\n",
      "Epoch [1430/4000], Loss: 45.1373\n",
      "RMSE: 45.9569\n",
      "Epoch [1440/4000], Loss: 45.2840\n",
      "RMSE: 46.0495\n",
      "Epoch [1450/4000], Loss: 45.2473\n",
      "RMSE: 45.9908\n",
      "Epoch [1460/4000], Loss: 45.1622\n",
      "RMSE: 45.9819\n",
      "Epoch [1470/4000], Loss: 45.1216\n",
      "RMSE: 45.9123\n",
      "Epoch [1480/4000], Loss: 45.4100\n",
      "RMSE: 46.0681\n",
      "Epoch [1490/4000], Loss: 45.2176\n",
      "RMSE: 45.9755\n",
      "Epoch [1500/4000], Loss: 45.2595\n",
      "RMSE: 45.9303\n",
      "Epoch [1510/4000], Loss: 45.1588\n",
      "RMSE: 45.9312\n",
      "Epoch [1520/4000], Loss: 45.2391\n",
      "RMSE: 45.9885\n",
      "Epoch [1530/4000], Loss: 45.1575\n",
      "RMSE: 45.9078\n",
      "Epoch [1540/4000], Loss: 45.3075\n",
      "RMSE: 46.2832\n",
      "Epoch [1550/4000], Loss: 45.1907\n",
      "RMSE: 45.9713\n",
      "Epoch [1560/4000], Loss: 45.1672\n",
      "RMSE: 45.9813\n",
      "Epoch [1570/4000], Loss: 45.0906\n",
      "RMSE: 45.9958\n",
      "Epoch [1580/4000], Loss: 45.1102\n",
      "RMSE: 45.9746\n",
      "Epoch [1590/4000], Loss: 45.1141\n",
      "RMSE: 45.9145\n",
      "Epoch [1600/4000], Loss: 45.1504\n",
      "RMSE: 45.9829\n",
      "Epoch [1610/4000], Loss: 45.1733\n",
      "RMSE: 45.9635\n",
      "Epoch [1620/4000], Loss: 45.1124\n",
      "RMSE: 45.9167\n",
      "Epoch [1630/4000], Loss: 45.0887\n",
      "RMSE: 45.9260\n",
      "Epoch [1640/4000], Loss: 45.2312\n",
      "RMSE: 45.9757\n",
      "Epoch [1650/4000], Loss: 45.1911\n",
      "RMSE: 46.0155\n",
      "Epoch [1660/4000], Loss: 45.1317\n",
      "RMSE: 45.8984\n",
      "Epoch [1670/4000], Loss: 45.1459\n",
      "RMSE: 45.8938\n",
      "Epoch [1680/4000], Loss: 45.2006\n",
      "RMSE: 45.9859\n",
      "Epoch [1690/4000], Loss: 45.1532\n",
      "RMSE: 45.9531\n",
      "Epoch [1700/4000], Loss: 45.0906\n",
      "RMSE: 45.9598\n",
      "Epoch [1710/4000], Loss: 45.1446\n",
      "RMSE: 45.9923\n",
      "Epoch [1720/4000], Loss: 45.2226\n",
      "RMSE: 45.9801\n",
      "Epoch [1730/4000], Loss: 45.1881\n",
      "RMSE: 45.9580\n",
      "Epoch [1740/4000], Loss: 45.1097\n",
      "RMSE: 45.9484\n",
      "Epoch [1750/4000], Loss: 45.1229\n",
      "RMSE: 45.9863\n",
      "Epoch [1760/4000], Loss: 45.2048\n",
      "RMSE: 46.0210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1770/4000], Loss: 45.2129\n",
      "RMSE: 45.9868\n",
      "Epoch [1780/4000], Loss: 45.1040\n",
      "RMSE: 45.9127\n",
      "Epoch [1790/4000], Loss: 45.1382\n",
      "RMSE: 45.8921\n",
      "Epoch [1800/4000], Loss: 45.1029\n",
      "RMSE: 45.9225\n",
      "Epoch [1810/4000], Loss: 45.0799\n",
      "RMSE: 45.8670\n",
      "Epoch [1820/4000], Loss: 45.1819\n",
      "RMSE: 46.0979\n",
      "Epoch [1830/4000], Loss: 45.1446\n",
      "RMSE: 45.9135\n",
      "Epoch [1840/4000], Loss: 45.0649\n",
      "RMSE: 45.9205\n",
      "Epoch [1850/4000], Loss: 45.1784\n",
      "RMSE: 45.9322\n",
      "Epoch [1860/4000], Loss: 45.0688\n",
      "RMSE: 45.9336\n",
      "Epoch [1870/4000], Loss: 45.0839\n",
      "RMSE: 45.9408\n",
      "Epoch [1880/4000], Loss: 45.0801\n",
      "RMSE: 45.9170\n",
      "Epoch [1890/4000], Loss: 45.0587\n",
      "RMSE: 45.9099\n",
      "Epoch [1900/4000], Loss: 45.0728\n",
      "RMSE: 45.9119\n",
      "Epoch [1910/4000], Loss: 45.0460\n",
      "RMSE: 45.8611\n",
      "Epoch [1920/4000], Loss: 45.0761\n",
      "RMSE: 45.9181\n",
      "Epoch [1930/4000], Loss: 45.0658\n",
      "RMSE: 45.9277\n",
      "Epoch [1940/4000], Loss: 45.0364\n",
      "RMSE: 45.8538\n",
      "Epoch [1950/4000], Loss: 45.1109\n",
      "RMSE: 45.9397\n",
      "Epoch [1960/4000], Loss: 45.0837\n",
      "RMSE: 45.9105\n",
      "Epoch [1970/4000], Loss: 45.0616\n",
      "RMSE: 45.8654\n",
      "Epoch [1980/4000], Loss: 45.0296\n",
      "RMSE: 45.9257\n",
      "Epoch [1990/4000], Loss: 45.1312\n",
      "RMSE: 45.9044\n",
      "Epoch [2000/4000], Loss: 45.1912\n",
      "RMSE: 45.9402\n",
      "Epoch [2010/4000], Loss: 45.2422\n",
      "RMSE: 46.1696\n",
      "Epoch [2020/4000], Loss: 45.1015\n",
      "RMSE: 45.9739\n",
      "Epoch [2030/4000], Loss: 45.0603\n",
      "RMSE: 45.8921\n",
      "Epoch [2040/4000], Loss: 45.0911\n",
      "RMSE: 45.9816\n",
      "Epoch [2050/4000], Loss: 45.1241\n",
      "RMSE: 45.9145\n",
      "Epoch [2060/4000], Loss: 45.1554\n",
      "RMSE: 45.9969\n",
      "Epoch [2070/4000], Loss: 45.0603\n",
      "RMSE: 45.9335\n",
      "Epoch [2080/4000], Loss: 45.0771\n",
      "RMSE: 45.9217\n",
      "Epoch [2090/4000], Loss: 45.0666\n",
      "RMSE: 45.8786\n",
      "Epoch [2100/4000], Loss: 45.1204\n",
      "RMSE: 45.9381\n",
      "Epoch [2110/4000], Loss: 45.1396\n",
      "RMSE: 45.8627\n",
      "Epoch [2120/4000], Loss: 45.2057\n",
      "RMSE: 45.8875\n",
      "Epoch [2130/4000], Loss: 45.0529\n",
      "RMSE: 45.9089\n",
      "Epoch [2140/4000], Loss: 45.0741\n",
      "RMSE: 45.9107\n",
      "Epoch [2150/4000], Loss: 45.0776\n",
      "RMSE: 45.8646\n",
      "Epoch [2160/4000], Loss: 45.0529\n",
      "RMSE: 45.9439\n",
      "Epoch [2170/4000], Loss: 45.1321\n",
      "RMSE: 45.8726\n",
      "Epoch [2180/4000], Loss: 45.0255\n",
      "RMSE: 45.8750\n",
      "Epoch [2190/4000], Loss: 45.0291\n",
      "RMSE: 45.8594\n",
      "Epoch [2200/4000], Loss: 45.0315\n",
      "RMSE: 45.8609\n",
      "Epoch [2210/4000], Loss: 45.0220\n",
      "RMSE: 45.8454\n",
      "Epoch [2220/4000], Loss: 45.0443\n",
      "RMSE: 46.0656\n",
      "Epoch [2230/4000], Loss: 45.2553\n",
      "RMSE: 45.9546\n",
      "Epoch [2240/4000], Loss: 45.1038\n",
      "RMSE: 45.9982\n",
      "Epoch [2250/4000], Loss: 45.0691\n",
      "RMSE: 45.8664\n",
      "Epoch [2260/4000], Loss: 45.0556\n",
      "RMSE: 45.9266\n",
      "Epoch [2270/4000], Loss: 45.0388\n",
      "RMSE: 45.8537\n",
      "Epoch [2280/4000], Loss: 45.1029\n",
      "RMSE: 45.9386\n",
      "Epoch [2290/4000], Loss: 45.0642\n",
      "RMSE: 45.9486\n",
      "Epoch [2300/4000], Loss: 45.1250\n",
      "RMSE: 45.9292\n",
      "Epoch [2310/4000], Loss: 45.1840\n",
      "RMSE: 45.9518\n",
      "Epoch [2320/4000], Loss: 45.0957\n",
      "RMSE: 45.8551\n",
      "Epoch [2330/4000], Loss: 45.1736\n",
      "RMSE: 45.9326\n",
      "Epoch [2340/4000], Loss: 45.0640\n",
      "RMSE: 45.8432\n",
      "Epoch [2350/4000], Loss: 45.1009\n",
      "RMSE: 45.9115\n",
      "Epoch [2360/4000], Loss: 45.0183\n",
      "RMSE: 45.8430\n",
      "Epoch [2370/4000], Loss: 45.0616\n",
      "RMSE: 45.8579\n",
      "Epoch [2380/4000], Loss: 45.0165\n",
      "RMSE: 45.8645\n",
      "Epoch [2390/4000], Loss: 45.0482\n",
      "RMSE: 45.8592\n",
      "Epoch [2400/4000], Loss: 45.0038\n",
      "RMSE: 45.8377\n",
      "Epoch [2410/4000], Loss: 45.1789\n",
      "RMSE: 45.9020\n",
      "Epoch [2420/4000], Loss: 45.0741\n",
      "RMSE: 45.9030\n",
      "Epoch [2430/4000], Loss: 45.0851\n",
      "RMSE: 45.9985\n",
      "Epoch [2440/4000], Loss: 45.0914\n",
      "RMSE: 46.0724\n",
      "Epoch [2450/4000], Loss: 45.0868\n",
      "RMSE: 46.0296\n",
      "Epoch [2460/4000], Loss: 45.1632\n",
      "RMSE: 45.9425\n",
      "Epoch [2470/4000], Loss: 45.0691\n",
      "RMSE: 45.9320\n",
      "Epoch [2480/4000], Loss: 45.0291\n",
      "RMSE: 45.8788\n",
      "Epoch [2490/4000], Loss: 45.0662\n",
      "RMSE: 45.8934\n",
      "Epoch [2500/4000], Loss: 45.0273\n",
      "RMSE: 45.8489\n",
      "Epoch [2510/4000], Loss: 45.0233\n",
      "RMSE: 45.8559\n",
      "Epoch [2520/4000], Loss: 45.0070\n",
      "RMSE: 45.8604\n",
      "Epoch [2530/4000], Loss: 45.0368\n",
      "RMSE: 45.8747\n",
      "Epoch [2540/4000], Loss: 44.9962\n",
      "RMSE: 45.8371\n",
      "Epoch [2550/4000], Loss: 45.0278\n",
      "RMSE: 45.8956\n",
      "Epoch [2560/4000], Loss: 45.1166\n",
      "RMSE: 45.9019\n",
      "Epoch [2570/4000], Loss: 45.1360\n",
      "RMSE: 45.9370\n",
      "Epoch [2580/4000], Loss: 45.0565\n",
      "RMSE: 45.8539\n",
      "Epoch [2590/4000], Loss: 45.0041\n",
      "RMSE: 45.8438\n",
      "Epoch [2600/4000], Loss: 45.0239\n",
      "RMSE: 45.8771\n",
      "Epoch [2610/4000], Loss: 44.9952\n",
      "RMSE: 45.8401\n",
      "Epoch [2620/4000], Loss: 44.9895\n",
      "RMSE: 45.8328\n",
      "Epoch [2630/4000], Loss: 45.0372\n",
      "RMSE: 45.8592\n",
      "Epoch [2640/4000], Loss: 45.0047\n",
      "RMSE: 45.9634\n",
      "Epoch [2650/4000], Loss: 45.1600\n",
      "RMSE: 45.9955\n",
      "Epoch [2660/4000], Loss: 45.1048\n",
      "RMSE: 45.8706\n",
      "Epoch [2670/4000], Loss: 45.0549\n",
      "RMSE: 45.8844\n",
      "Epoch [2680/4000], Loss: 45.0409\n",
      "RMSE: 45.8691\n",
      "Epoch [2690/4000], Loss: 45.0084\n",
      "RMSE: 45.8552\n",
      "Epoch [2700/4000], Loss: 45.1694\n",
      "RMSE: 46.0378\n",
      "Epoch [2710/4000], Loss: 45.4196\n",
      "RMSE: 46.2169\n",
      "Epoch [2720/4000], Loss: 45.1490\n",
      "RMSE: 46.0288\n",
      "Epoch [2730/4000], Loss: 45.1124\n",
      "RMSE: 46.0073\n",
      "Epoch [2740/4000], Loss: 46.6337\n",
      "RMSE: 46.2363\n",
      "Epoch [2750/4000], Loss: 45.4677\n",
      "RMSE: 46.3825\n",
      "Epoch [2760/4000], Loss: 45.3871\n",
      "RMSE: 46.1408\n",
      "Epoch [2770/4000], Loss: 45.1560\n",
      "RMSE: 45.9558\n",
      "Epoch [2780/4000], Loss: 45.0782\n",
      "RMSE: 45.9179\n",
      "Epoch [2790/4000], Loss: 45.0674\n",
      "RMSE: 45.9029\n",
      "Epoch [2800/4000], Loss: 45.0482\n",
      "RMSE: 45.8866\n",
      "Epoch [2810/4000], Loss: 45.0441\n",
      "RMSE: 45.8769\n",
      "Epoch [2820/4000], Loss: 45.0651\n",
      "RMSE: 45.9012\n",
      "Epoch [2830/4000], Loss: 45.0314\n",
      "RMSE: 45.8716\n",
      "Epoch [2840/4000], Loss: 45.0820\n",
      "RMSE: 45.9090\n",
      "Epoch [2850/4000], Loss: 45.0614\n",
      "RMSE: 45.9006\n",
      "Epoch [2860/4000], Loss: 45.0298\n",
      "RMSE: 45.8634\n",
      "Epoch [2870/4000], Loss: 45.0338\n",
      "RMSE: 45.8882\n",
      "Epoch [2880/4000], Loss: 45.0437\n",
      "RMSE: 45.8662\n",
      "Epoch [2890/4000], Loss: 45.0522\n",
      "RMSE: 45.9134\n",
      "Epoch [2900/4000], Loss: 45.0164\n",
      "RMSE: 45.8647\n",
      "Epoch [2910/4000], Loss: 45.0196\n",
      "RMSE: 45.8597\n",
      "Epoch [2920/4000], Loss: 45.0062\n",
      "RMSE: 45.8502\n",
      "Epoch [2930/4000], Loss: 45.0316\n",
      "RMSE: 45.8985\n",
      "Epoch [2940/4000], Loss: 45.0736\n",
      "RMSE: 45.9506\n",
      "Epoch [2950/4000], Loss: 45.0386\n",
      "RMSE: 45.8946\n",
      "Epoch [2960/4000], Loss: 45.0168\n",
      "RMSE: 45.8631\n",
      "Epoch [2970/4000], Loss: 45.0038\n",
      "RMSE: 45.8692\n",
      "Epoch [2980/4000], Loss: 45.1849\n",
      "RMSE: 45.9715\n",
      "Epoch [2990/4000], Loss: 45.0879\n",
      "RMSE: 45.9440\n",
      "Epoch [3000/4000], Loss: 45.0014\n",
      "RMSE: 45.8621\n",
      "Epoch [3010/4000], Loss: 45.0196\n",
      "RMSE: 45.8771\n",
      "Epoch [3020/4000], Loss: 44.9883\n",
      "RMSE: 45.8445\n",
      "Epoch [3030/4000], Loss: 45.0429\n",
      "RMSE: 45.9922\n",
      "Epoch [3040/4000], Loss: 45.0008\n",
      "RMSE: 45.8695\n",
      "Epoch [3050/4000], Loss: 45.0153\n",
      "RMSE: 45.8549\n",
      "Epoch [3060/4000], Loss: 45.0110\n",
      "RMSE: 45.8536\n",
      "Epoch [3070/4000], Loss: 44.9791\n",
      "RMSE: 45.8312\n",
      "Epoch [3080/4000], Loss: 45.1457\n",
      "RMSE: 45.9898\n",
      "Epoch [3090/4000], Loss: 45.0352\n",
      "RMSE: 46.0483\n",
      "Epoch [3100/4000], Loss: 45.1364\n",
      "RMSE: 45.9286\n",
      "Epoch [3110/4000], Loss: 45.0838\n",
      "RMSE: 45.9036\n",
      "Epoch [3120/4000], Loss: 45.0292\n",
      "RMSE: 45.8879\n",
      "Epoch [3130/4000], Loss: 44.9817\n",
      "RMSE: 45.8550\n",
      "Epoch [3140/4000], Loss: 44.9839\n",
      "RMSE: 45.8387\n",
      "Epoch [3150/4000], Loss: 45.0011\n",
      "RMSE: 45.8735\n",
      "Epoch [3160/4000], Loss: 44.9918\n",
      "RMSE: 45.8642\n",
      "Epoch [3170/4000], Loss: 44.9915\n",
      "RMSE: 45.8646\n",
      "Epoch [3180/4000], Loss: 45.0167\n",
      "RMSE: 45.8895\n",
      "Epoch [3190/4000], Loss: 45.0322\n",
      "RMSE: 45.8678\n",
      "Epoch [3200/4000], Loss: 45.0610\n",
      "RMSE: 45.8932\n",
      "Epoch [3210/4000], Loss: 44.9790\n",
      "RMSE: 45.8257\n",
      "Epoch [3220/4000], Loss: 45.0244\n",
      "RMSE: 45.8525\n",
      "Epoch [3230/4000], Loss: 45.0118\n",
      "RMSE: 45.8647\n",
      "Epoch [3240/4000], Loss: 44.9783\n",
      "RMSE: 45.8337\n",
      "Epoch [3250/4000], Loss: 44.9710\n",
      "RMSE: 45.8373\n",
      "Epoch [3260/4000], Loss: 45.3637\n",
      "RMSE: 46.9494\n",
      "Epoch [3270/4000], Loss: 45.4931\n",
      "RMSE: 46.0899\n",
      "Epoch [3280/4000], Loss: 45.6469\n",
      "RMSE: 46.1744\n",
      "Epoch [3290/4000], Loss: 45.5934\n",
      "RMSE: 46.1366\n",
      "Epoch [3300/4000], Loss: 45.0975\n",
      "RMSE: 46.0526\n",
      "Epoch [3310/4000], Loss: 45.0708\n",
      "RMSE: 45.9697\n",
      "Epoch [3320/4000], Loss: 45.0699\n",
      "RMSE: 45.9203\n",
      "Epoch [3330/4000], Loss: 45.0405\n",
      "RMSE: 45.9192\n",
      "Epoch [3340/4000], Loss: 45.0304\n",
      "RMSE: 45.9071\n",
      "Epoch [3350/4000], Loss: 45.0404\n",
      "RMSE: 45.9293\n",
      "Epoch [3360/4000], Loss: 45.0272\n",
      "RMSE: 45.9029\n",
      "Epoch [3370/4000], Loss: 45.0481\n",
      "RMSE: 45.9308\n",
      "Epoch [3380/4000], Loss: 45.0405\n",
      "RMSE: 45.9038\n",
      "Epoch [3390/4000], Loss: 45.0343\n",
      "RMSE: 45.8933\n",
      "Epoch [3400/4000], Loss: 45.0196\n",
      "RMSE: 45.8700\n",
      "Epoch [3410/4000], Loss: 45.0080\n",
      "RMSE: 45.8714\n",
      "Epoch [3420/4000], Loss: 44.9950\n",
      "RMSE: 45.8527\n",
      "Epoch [3430/4000], Loss: 45.0002\n",
      "RMSE: 45.8587\n",
      "Epoch [3440/4000], Loss: 44.9825\n",
      "RMSE: 45.8432\n",
      "Epoch [3450/4000], Loss: 45.0083\n",
      "RMSE: 45.8695\n",
      "Epoch [3460/4000], Loss: 44.9849\n",
      "RMSE: 45.8611\n",
      "Epoch [3470/4000], Loss: 44.9774\n",
      "RMSE: 45.8409\n",
      "Epoch [3480/4000], Loss: 44.9990\n",
      "RMSE: 45.8473\n",
      "Epoch [3490/4000], Loss: 44.9713\n",
      "RMSE: 45.8417\n",
      "Epoch [3500/4000], Loss: 44.9948\n",
      "RMSE: 45.8654\n",
      "Epoch [3510/4000], Loss: 44.9623\n",
      "RMSE: 45.8272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3520/4000], Loss: 44.9951\n",
      "RMSE: 45.8740\n",
      "Epoch [3530/4000], Loss: 45.0307\n",
      "RMSE: 45.9092\n",
      "Epoch [3540/4000], Loss: 45.0557\n",
      "RMSE: 45.9364\n",
      "Epoch [3550/4000], Loss: 45.0472\n",
      "RMSE: 45.8662\n",
      "Epoch [3560/4000], Loss: 45.0942\n",
      "RMSE: 46.1681\n",
      "Epoch [3570/4000], Loss: 45.0624\n",
      "RMSE: 45.9572\n",
      "Epoch [3580/4000], Loss: 45.0859\n",
      "RMSE: 45.8951\n",
      "Epoch [3590/4000], Loss: 45.0224\n",
      "RMSE: 45.9540\n",
      "Epoch [3600/4000], Loss: 44.9849\n",
      "RMSE: 45.8696\n",
      "Epoch [3610/4000], Loss: 45.0013\n",
      "RMSE: 45.8850\n",
      "Epoch [3620/4000], Loss: 44.9600\n",
      "RMSE: 45.8458\n",
      "Epoch [3630/4000], Loss: 44.9900\n",
      "RMSE: 45.8590\n",
      "Epoch [3640/4000], Loss: 44.9865\n",
      "RMSE: 45.8757\n",
      "Epoch [3650/4000], Loss: 44.9584\n",
      "RMSE: 45.8302\n",
      "Epoch [3660/4000], Loss: 45.0130\n",
      "RMSE: 45.8454\n",
      "Epoch [3670/4000], Loss: 44.9829\n",
      "RMSE: 45.8407\n",
      "Epoch [3680/4000], Loss: 44.9724\n",
      "RMSE: 45.8932\n",
      "Epoch [3690/4000], Loss: 45.5468\n",
      "RMSE: 46.6666\n",
      "Epoch [3700/4000], Loss: 45.0464\n",
      "RMSE: 46.0137\n",
      "Epoch [3710/4000], Loss: 46.5174\n",
      "RMSE: 46.3470\n",
      "Epoch [3720/4000], Loss: 48.0186\n",
      "RMSE: 47.2128\n",
      "Epoch [3730/4000], Loss: 45.5645\n",
      "RMSE: 46.3075\n",
      "Epoch [3740/4000], Loss: 46.1798\n",
      "RMSE: 46.7835\n",
      "Epoch [3750/4000], Loss: 45.4342\n",
      "RMSE: 46.3377\n",
      "Epoch [3760/4000], Loss: 45.6168\n",
      "RMSE: 46.2447\n",
      "Epoch [3770/4000], Loss: 45.3241\n",
      "RMSE: 46.0629\n",
      "Epoch [3780/4000], Loss: 45.1449\n",
      "RMSE: 45.9941\n",
      "Epoch [3790/4000], Loss: 45.0923\n",
      "RMSE: 45.8928\n",
      "Epoch [3800/4000], Loss: 45.0400\n",
      "RMSE: 45.8808\n",
      "Epoch [3810/4000], Loss: 45.0220\n",
      "RMSE: 45.8751\n",
      "Epoch [3820/4000], Loss: 45.0128\n",
      "RMSE: 45.8630\n",
      "Epoch [3830/4000], Loss: 45.0071\n",
      "RMSE: 45.8605\n",
      "Epoch [3840/4000], Loss: 45.0023\n",
      "RMSE: 45.8597\n",
      "Epoch [3850/4000], Loss: 44.9980\n",
      "RMSE: 45.8543\n",
      "Epoch [3860/4000], Loss: 44.9943\n",
      "RMSE: 45.8524\n",
      "Epoch [3870/4000], Loss: 44.9910\n",
      "RMSE: 45.8485\n",
      "Epoch [3880/4000], Loss: 44.9878\n",
      "RMSE: 45.8512\n",
      "Epoch [3890/4000], Loss: 44.9844\n",
      "RMSE: 45.8465\n",
      "Epoch [3900/4000], Loss: 44.9828\n",
      "RMSE: 45.8424\n",
      "Epoch [3910/4000], Loss: 44.9795\n",
      "RMSE: 45.8460\n",
      "Epoch [3920/4000], Loss: 44.9779\n",
      "RMSE: 45.8420\n",
      "Epoch [3930/4000], Loss: 45.0252\n",
      "RMSE: 45.8908\n",
      "Epoch [3940/4000], Loss: 44.9984\n",
      "RMSE: 45.8605\n",
      "Epoch [3950/4000], Loss: 44.9907\n",
      "RMSE: 45.8627\n",
      "Epoch [3960/4000], Loss: 44.9691\n",
      "RMSE: 45.8413\n",
      "Epoch [3970/4000], Loss: 44.9765\n",
      "RMSE: 45.8308\n",
      "Epoch [3980/4000], Loss: 45.0011\n",
      "RMSE: 45.8701\n",
      "Epoch [3990/4000], Loss: 44.9729\n",
      "RMSE: 45.8427\n",
      "Epoch [4000/4000], Loss: 44.9607\n",
      "RMSE: 45.8294\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(50, 50)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(50, 50)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(50, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def testV(m):\n",
    "    Y_pred = m(X_test)\n",
    "    rmse = mean_squared_error(y_test.cpu(), Y_pred.cpu().numpy())**0.5\n",
    "    print('RMSE: {:.4f}'.format(rmse))\n",
    "\n",
    "# Define the training function\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def ttrain(model, X, Y, optimizer, criterion, num_epochs):\n",
    "    loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward pass\n",
    "        outputs = model(X)\n",
    "        loss = criterion(outputs, Y)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        lval = float(torch.sqrt(loss))\n",
    "        loss_history.append(lval)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, lval))\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            Y_pred = model(X_test)\n",
    "            rmse = mean_squared_error(y_test.cpu(), Y_pred.cpu().detach().numpy())**0.5\n",
    "            print('RMSE: {:.4f}'.format(rmse))\n",
    "    \n",
    "    # Plot the loss history\n",
    "    plt.plot(loss_history)\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "# Initialize the neural network and set hyperparameters\n",
    "model = Net()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0075)\n",
    "num_epochs = 4000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train['Week_of_Year'] = train['Week_of_Year'].astype('int64')\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, y, train_size=0.99, shuffle=True)\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    # Move the tensors to the GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_test = X_test.to(device)\n",
    "    y_test = y_test.to(device)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "ttrain(model, X_train, y_train, optimizer, criterion, num_epochs)\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "# with torch.no_grad():\n",
    "#     Y_pred = model(X_test)\n",
    "#     rmse = mean_squared_error(y_test, Y_pred)**0.5\n",
    "#     print('RMSE: {:.4f}'.format(rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "414a858f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALL_TYPE          int8\n",
      "ORIGIN_CALL       int32\n",
      "ORIGIN_STAND       int8\n",
      "TAXI_ID           int16\n",
      "Day_of_Year       int64\n",
      "Day_of_Week       int64\n",
      "Hour_of_Day       int64\n",
      "Minute_of_Hour    int64\n",
      "Week_of_Year      int64\n",
      "Segment_of_Day    int64\n",
      "dtype: object\n",
      "int64\n",
      "0    23\n",
      "1    19\n",
      "2    65\n",
      "3    43\n",
      "4    29\n",
      "Name: TIME, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train.dtypes)\n",
    "print(y.dtypes)\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93cfe2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test['Week_of_Year'] = test['Week_of_Year'].astype('int64')\n",
    "\n",
    "\n",
    "test_data = torch.tensor(test.values, dtype=torch.float32)\n",
    "test_data = test_data.to(device)\n",
    "y_pred = model(test_data)\n",
    "y_pred_np = y_pred.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# print(y_pred)\n",
    "\n",
    "# print(type(y_pred))\n",
    "# print(type(testids), testids.columns.values)\n",
    "# testids = testids.to_frame(\"TRIP_ID\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_ids_real = testids.to_frame(name='TRIP_ID')\n",
    "\n",
    "\n",
    "test_times = pd.DataFrame(y_pred_np, columns=['TRAVEL_TIME'])\n",
    "\n",
    "df_combined = pd.concat([test_ids_real, test_times], axis=1)\n",
    "\n",
    "df_combined.to_csv('combined_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
